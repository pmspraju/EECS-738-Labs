{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the IMDB data from keras for this lab. The data is already enocded so I wanted to show an example of what text data looks like before it gets encoded. Below is the stanford sentiment treebank data broken up into its data and the sentiment values.\n",
    "\n",
    "I wanted to use a more complex dataset for this but the time constraints due to COV-19 have made that difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>phrase_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! '</td>\n",
       "      <td>22935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! ''</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>179257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>22936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>zoning ordinances to protect your community fr...</td>\n",
       "      <td>220441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "      <td>179256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>élan</td>\n",
       "      <td>220442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>É</td>\n",
       "      <td>220443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>É um passatempo descompromissado</td>\n",
       "      <td>220444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Phrase phrase_ids\n",
       "0                                                       !          0\n",
       "1                                                     ! '      22935\n",
       "2                                                    ! ''      18235\n",
       "3                                                  ! Alas     179257\n",
       "4                                             ! Brilliant      22936\n",
       "...                                                   ...        ...\n",
       "239227  zoning ordinances to protect your community fr...     220441\n",
       "239228                                          zzzzzzzzz     179256\n",
       "239229                                               élan     220442\n",
       "239230                                                  É     220443\n",
       "239231                   É um passatempo descompromissado     220444\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I import it as a table using pandas and I have to set the column names manually.\n",
    "data = pd.read_table('dictionary.txt',index_col=False, header=None)\n",
    "data.columns = ['Phrase|Index']\n",
    "# Most corpus text data is set up in a way that has delimiters to seperate the data from the indexs. The delimiter for this data is |\n",
    "data = data['Phrase|Index'].str.split('|', expand=True)\n",
    "# The code above split the strings into an id index and the actual data.\n",
    "data = data.rename(columns={0: 'Phrase', 1: 'phrase_ids'})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_ids</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.42708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>239227</td>\n",
       "      <td>0.36111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>239228</td>\n",
       "      <td>0.38889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>239229</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>239230</td>\n",
       "      <td>0.88889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>239231</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       phrase_ids sentiment_values\n",
       "0               0              0.5\n",
       "1               1              0.5\n",
       "2               2          0.44444\n",
       "3               3              0.5\n",
       "4               4          0.42708\n",
       "...           ...              ...\n",
       "239227     239227          0.36111\n",
       "239228     239228          0.38889\n",
       "239229     239229          0.33333\n",
       "239230     239230          0.88889\n",
       "239231     239231              0.5\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I then do the same steps for the sentiment value labels for this data.\n",
    "labels = pd.read_table('sentiment_labels.txt')\n",
    "labels = labels['phrase ids|sentiment values'].str.split('|', expand=True)\n",
    "labels = labels.rename(columns={0: 'phrase_ids', 1: 'sentiment_values'})\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase_ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22935</th>\n",
       "      <td>! '</td>\n",
       "      <td>0.52778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18235</th>\n",
       "      <td>! ''</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179257</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22936</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>0.86111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220441</th>\n",
       "      <td>zoning ordinances to protect your community fr...</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179256</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "      <td>0.19444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220442</th>\n",
       "      <td>élan</td>\n",
       "      <td>0.51389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220443</th>\n",
       "      <td>É</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220444</th>\n",
       "      <td>É um passatempo descompromissado</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Phrase sentiment_values\n",
       "phrase_ids                                                                    \n",
       "0                                                           !              0.5\n",
       "22935                                                     ! '          0.52778\n",
       "18235                                                    ! ''              0.5\n",
       "179257                                                 ! Alas          0.44444\n",
       "22936                                             ! Brilliant          0.86111\n",
       "...                                                       ...              ...\n",
       "220441      zoning ordinances to protect your community fr...          0.13889\n",
       "179256                                              zzzzzzzzz          0.19444\n",
       "220442                                                   élan          0.51389\n",
       "220443                                                      É              0.5\n",
       "220444                       É um passatempo descompromissado              0.5\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I merged the data into one table using a SQL like join command which combines the tables based on the 'phrase_ids' column.\n",
    "df = data.merge(labels, how='inner', on='phrase_ids')\n",
    "# I then set the phrase ids as the index for the dataframe and drop the redundant id column.\n",
    "df.index = df.phrase_ids\n",
    "df = df.drop('phrase_ids', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above process is very common and you should practice it yourself to get used to the process. I recommend (this isn't graded) to do this yourself with the amazon review data here: http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pretrained embedder called GloVe. here I had to use the 'quoting=csv.QUOTE_NONE' because the text file included quotation marks.\n",
    "embed = pd.read_table('glove.6B.100d.txt', engine='python', encoding='utf-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the -0.038194 -0.24487 0.72812 -0.39961 0.0831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>, -0.10767 0.11053 0.59812 -0.54361 0.67396 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>. -0.33979 0.20941 0.46348 -0.64792 -0.38377 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of -0.1529 -0.24279 0.89837 0.16996 0.53516 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to -0.1897 0.050024 0.19084 -0.049184 -0.08973...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>chanty -0.15577 -0.049188 -0.064377 0.2236 -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>kronik -0.094426 0.14725 -0.15739 0.071966 -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>rolonda 0.36088 -0.16919 -0.32704 0.098332 -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>zsombor -0.10461 -0.5047 -0.49331 0.13516 -0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>sandberger 0.28365 -0.6263 -0.44351 0.2177 -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0       the -0.038194 -0.24487 0.72812 -0.39961 0.0831...\n",
       "1       , -0.10767 0.11053 0.59812 -0.54361 0.67396 0....\n",
       "2       . -0.33979 0.20941 0.46348 -0.64792 -0.38377 0...\n",
       "3       of -0.1529 -0.24279 0.89837 0.16996 0.53516 0....\n",
       "4       to -0.1897 0.050024 0.19084 -0.049184 -0.08973...\n",
       "...                                                   ...\n",
       "399995  chanty -0.15577 -0.049188 -0.064377 0.2236 -0....\n",
       "399996  kronik -0.094426 0.14725 -0.15739 0.071966 -0....\n",
       "399997  rolonda 0.36088 -0.16919 -0.32704 0.098332 -0....\n",
       "399998  zsombor -0.10461 -0.5047 -0.49331 0.13516 -0.3...\n",
       "399999  sandberger 0.28365 -0.6263 -0.44351 0.2177 -0....\n",
       "\n",
       "[400000 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see here that the data has two components to it. The beginning of every string is the word that got embedded and it is followed by a long list of numbers.\n",
    "# These numbers are how the machine learning model trains on data. Normally you will automatically create a trainable embedding in your model (like we will today).\n",
    "# There are times when you will want to use a pretrained embedding like GloVe to speed up computation time. So its good to know how to use them.\n",
    "# You can find detailed instructions on how to use GloVe and other pre-trained embeddings in the keras and tensorflow docs.\n",
    "embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets get into the data we are working with today. In the last couple of labs we used CNNs and ResNets a lot. This time we are going to compare CNNs with LSTMs for the purpose of classifying text. The data is setup so that a '0 label' is a negative review and a '1 label' is a postive review.\n",
    "\n",
    "We want to create machine learning models to automatically detect whether or not a review is positive. This has wide applications for both industry and research and has been extensively researched since 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load our data. We will limit the number of words to 5,000 as that is how the data is setup.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "# We pad the data because not all sentences in our data are the same length. We want to use a number that is larger than our largest data. Here I will choose 400.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=400)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with a very simple 1D CNN model. We will use this as our baseline for everything else in this lab.\n",
    "model = Sequential()\n",
    "\n",
    "# This embedding is a trainable parameter. We aren't using GloVE for this model.\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# There isn't much of a difference with how 1D and 2D CNNs work. They still use filters and scan the data.\n",
    "# we will use a similar model as our 2D CNN with the adition of an embedding layer at the beginning.\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We will use a sigmoid and a 1 neuron dense output since our data is binary.\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How well is this model doing? Is it overfitting? If so how could you fix this since we are already applying BatchNorm and dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and apply those fixes here. Can you make a baseline that doesn't overfit? What worked best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make the network more complex by adding more filters to the data. How did this affect training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets add more CNN and BatchNorm layers to the network. Did this have the same affect as 2D CNNs from lab 5?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do 1D CNNs and 2D CNNs behave the same from the changes we are making?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at some LSTMs. LSTMs and RNNs in general were the racehorse of deep learning from 2014-2016. Now they have drastically fallen off of favor in the DL community. The questions we want to answer in this lab are: Why do you think this is? Do you think it was a mistake to stray away from RNNs? What changes do you think we could make to make them better or should we just drop them all together?\n",
    "\n",
    "The resources to learn more about this debate can be found here: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0\n",
    "\n",
    "and here: https://towardsdatascience.com/memory-attention-sequences-37456d271992\n",
    "\n",
    "and here: https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c\n",
    "\n",
    "These are optional readings but they serve to give you a firm foundation on the knowledge of current deep learning thought. Feel free to answer the above questions after we train our LSTMs.\n",
    "\n",
    "If you don't know anything about RNNs read this: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make our LSTMs. We will use a smaller batch size as they take longer to train.\n",
    "# We use the same embedding layers as we did for our CNNs.\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "\n",
    "# Here we will add in our LSTM layers. They should be directly after the embedding layer.\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Now we will cast the LSTM output to a dense layer to sort it. If you haven't noticed, thick dense layers at the end of networks are how every model 'collects its thoughts'.\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the basic LSTM compare to the 1D CNN? Is it overfitting as much? is it's testing accuracy better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets add another LSTM layer to our model. Did that improve overfitting/accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets use larger LSTM layers. What affect did that have? Why do you think that is based off of your knowledge of RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets add Bi-directional layers to each of our RNNs. These make the model learn the data scanning both forwards and backwards. \n",
    "# Here is a detailed description: https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "\n",
    "# The bidirectional layer is a wrapper, you can apply it like so to each LSTM layer.\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "#How does this affect training/overfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you think of anyway to prevent overfitting in an LSTM? got down some ideas and feel free to try them. If you get a signifcant result post it to the discussion board for the rest of the class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at the classical examples of 1D CNNs and LSTMs, what do you think are the potential tradeoffs between using each one? Which one makes more sense to use and is there a reason to use LSTMs or RNNs in general for sequential data?\n",
    "\n",
    "If you are feeling brave and have the extra time I encourage you to impliment an attention layer for both the 1D CNN and bi-directional LSTM and see how much Attention helps. You can also use image attention layers to improve 2D CNNs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
